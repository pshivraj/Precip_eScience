{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what's the idea here? \n",
    "\n",
    "* Lauren wrote this original notebook\n",
    "* Rob prepended a breakdown of the data netcdf files on 15-12-2018\n",
    "  * += Shiv's ftp copy code to grab a netcdf file to the local file system\n",
    "    * Note username and token are taken from a non-repo location '../creds/trmm_creds'\n",
    "    * Target directory is also outside the repo '../data/trmm'\n",
    "    * The data file can be reloaded easily\n",
    "  * Then pull that apart a bit to understand the guts of the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell: Only to transfer an example file from UW Atmospheric Sciences ftp server to a local directory. \n",
    "run_this_cell = False\n",
    "\n",
    "if run_this_cell: \n",
    "    import requests\n",
    "    \n",
    "    # get a modest 594kb NetCDF file\n",
    "    source_url = \"http://trmm.atmos.washington.edu/EPO/interp_data/1999/01/TPR7_uw1_06294.19990101.002101_EPO.nc4\"\n",
    "      # trmm.atmos.washington.edu is the ftp server \n",
    "      # EPO is East Pacific Ocean\n",
    "      # interp_data is probably interpolated data, heh heh\n",
    "      # 06294 is an orbit number. Notice that this matches the date nicely from the launch date Nov 27 1997\n",
    "      # 19910101 is Jan 1 1999\n",
    "      # 002101 is hour 0 minute 21 second 01\n",
    "    target_filename = '../data/trmm/test.nc4'\n",
    "    authfile=open('../creds/trmm_creds','r')       # format of this file is username,token\n",
    "    a=authfile.readline().rstrip().split(',')      # rstrip() removes any trailing \\n whitespace; split returns a list\n",
    "    authdata = (a[0],a[1])                         # ...so we enumerate tuple from that list\n",
    "    authfile.close()\n",
    "    r = requests.get(source_url, auth = authdata,stream = True)\n",
    "    if r.status_code == 200:\n",
    "        with open(target_filename, 'wb') as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xarray Dataset breakdown\n",
    "\n",
    "In what follows the dataset **ds** is printed and shows **Dimensions**, **Coordinates**, **Data variables** and **Attributes**.\n",
    "The Attributes are self-explanatory. The Dimensions are *available* dimensions but each Data variable has its own subset of \n",
    "these. For example there are two vertical scales: One for latent heating (19 values) and one that is used for corr_Zfactor.\n",
    "So really the place to start looking is in the Data variables section. Data for each data variable can be printed as a\n",
    "multi-dimensional data box using this formula:\n",
    "\n",
    "\n",
    "```\n",
    "print(ds['Data_variable_name'][A:B, C:D, E:F].values)\n",
    "```\n",
    "\n",
    "In this case that particular Data variable had three indices. See below, the latent heat example, which has four\n",
    "indices: time, altitude_lh, latitude, longitude. By the way the time index comes along for the ride as an inconvenience; \n",
    "there is only one time index value which is zero; so in fact for latent heat we have square bracket indexing\n",
    "\\[0, A:B, C:D, E:F\\] where the time index is first, then latent heat index, then lat, then lon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:             (altitude: 80, altitude_lh: 19, latitude: 56, longitude: 309, time: 1)\n",
       "Coordinates:\n",
       "  * time                (time) datetime64[ns] 1999-01-01T00:21:01\n",
       "  * longitude           (longitude) float32 -175.65 -175.6 -175.55 -175.5 ...\n",
       "  * latitude            (latitude) float32 33.3 33.35 33.4 33.45 33.5 33.55 ...\n",
       "  * altitude            (altitude) float32 0.0 0.25 0.5 0.75 1.0 1.25 1.5 ...\n",
       "  * altitude_lh         (altitude_lh) float32 0.0 0.5 1.0 2.0 3.0 4.0 5.0 ...\n",
       "Data variables:\n",
       "    corr_Zfactor        (time, altitude, latitude, longitude) float32 dask.array<shape=(1, 80, 56, 309), chunksize=(1, 80, 56, 309)>\n",
       "    rain_type           (time, latitude, longitude) float32 dask.array<shape=(1, 56, 309), chunksize=(1, 56, 309)>\n",
       "    rain_type_original  (time, latitude, longitude) float64 dask.array<shape=(1, 56, 309), chunksize=(1, 56, 309)>\n",
       "    surf_rain           (time, latitude, longitude) float32 dask.array<shape=(1, 56, 309), chunksize=(1, 56, 309)>\n",
       "    swath               (time, latitude, longitude) float32 dask.array<shape=(1, 56, 309), chunksize=(1, 56, 309)>\n",
       "    latent_heating      (time, altitude_lh, latitude, longitude) float32 dask.array<shape=(1, 19, 56, 309), chunksize=(1, 19, 56, 309)>\n",
       "Attributes:\n",
       "    title:          Orbital interpolated TRMM PR 2A25, 2A23 and 2H25 v7 data\n",
       "    source_uw:      Created by the Mesoscale Group, University of Washington\n",
       "    source_nasa:    Original data obtained from NASA Goddard Earth Sciences h...\n",
       "    reference:      Houze et al. 2015 (Reviews of Geophysics)\n",
       "    data_location:  http://trmm.atmos.washington.edu/\n",
       "    orbit:          06294\n",
       "    region:         EPO East Pacific Ocean\n",
       "    lon_min:        -180.0\n",
       "    lon_max:        -120.0\n",
       "    lat_min:        -40.0\n",
       "    lat_max:        40.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "data_dir = '../data/trmm/'\n",
    "trmm_filename = data_dir + 'test.nc4'\n",
    "# print('\\nI am running Python {}...'.format(sys.version_info[0]))\n",
    "ds = xr.open_mfdataset(trmm_filename)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbital interpolated TRMM PR 2A25, 2A23 and 2H25 v7 data\n",
      "Created by the Mesoscale Group, University of Washington\n",
      "Original data obtained from NASA Goddard Earth Sciences http://trmm.gsfc.nasa.gov/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at some attributes \n",
    "print(ds.title)\n",
    "print(ds.source_uw)\n",
    "print(ds.source_nasa + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 versus 309 elements; so this agrees\n"
     ]
    }
   ],
   "source": [
    "# Now to look at the longitude coordinate: It is not perfect but close enough to regularly spaced\n",
    "# print(ds.longitude)\n",
    "# print('\\n')\n",
    "dlon = float(ds.longitude[1]-ds.longitude[0])    # this is almost but not quite 0.05 degrees\n",
    "# This produces a 0-dimensional (?) DataArray: dlon1 = ds.longitude[0]-ds.longitude[1]\n",
    "print(int((ds.longitude[-1] - ds.longitude[0])/dlon) + 1, 'versus', len(ds.longitude), 'elements; so this agrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 versus 56 elements; so this agrees\n"
     ]
    }
   ],
   "source": [
    "# print(ds.latitude, '\\n')\n",
    "# dlat = float(ds.latitude[1]-ds.latitude[0])\n",
    "# print(dlat)\n",
    "dlat = float(ds.latitude[-1]-ds.latitude[-2])\n",
    "# print(dlat) # both edges of the latitude array give a delta of 0.049999237060546875 deg\n",
    "print(int((ds.latitude[-1] - ds.latitude[0])/dlat) + 1, 'versus', len(ds.latitude), 'elements; so this agrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'altitude_lh' (altitude_lh: 19)>\n",
      "array([ 0. ,  0.5,  1. ,  2. ,  3. ,  4. ,  5. ,  6. ,  7. ,  8. ,  9. , 10. ,\n",
      "       11. , 12. , 13. , 14. , 15. , 16. , 17. ], dtype=float32)\n",
      "Coordinates:\n",
      "  * altitude_lh  (altitude_lh) float32 0.0 0.5 1.0 2.0 3.0 4.0 5.0 6.0 7.0 ...\n",
      "Attributes:\n",
      "    units:      km\n",
      "    long_name:  altitude for latent heating MSL\n"
     ]
    }
   ],
   "source": [
    "# print(ds.altitude) will give 80 values; the latent heat gives 19 of them\n",
    "print(ds.altitude_lh) # lh is latent heating; units are km above sea level \n",
    "                      # latent heat is a change in internal energy without a change in temperature\n",
    "                      #   energy needed in a phase transition, specifically surface water to water vapor via\n",
    "                      #   evaporation / transpiration and vice versa condensation from water vapor to leetle \n",
    "                      #   drops of rain... should get more perspective from Lauren here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.        0.        0.        0.       -0.287714  0.      ]\n",
      "  [ 0.        0.        0.       -0.2258   -0.610915  0.      ]\n",
      "  [ 0.        0.       -0.238519 -0.084865 -0.724914 -0.679886]\n",
      "  [-0.471906 -0.341696 -0.721401 -0.716218 -0.850921 -0.691996]\n",
      "  [-0.136095 -0.380722 -0.06086  -0.67395  -0.911399 -0.629777]]\n",
      "\n",
      " [[ 0.        0.        0.        0.       -0.215227  0.      ]\n",
      "  [ 0.        0.        0.       -0.184009 -0.464301  0.      ]\n",
      "  [ 0.        0.       -0.186845 -0.068707 -0.531694 -0.494573]\n",
      "  [-0.387039 -0.279528 -0.536328 -0.529532 -0.610049 -0.525723]\n",
      "  [-0.100541 -0.286826 -0.038843 -0.48359  -0.644995 -0.449854]]\n",
      "\n",
      " [[ 0.        0.        0.        0.        0.12937   0.      ]\n",
      "  [ 0.        0.        0.        0.043131  0.198275  0.      ]\n",
      "  [ 0.        0.        0.088941  0.044162  0.325596  0.360193]\n",
      "  [ 0.090915  0.075767  0.27138   0.283534  0.434444  0.239589]\n",
      "  [ 0.045268  0.131749  0.019471  0.337683  0.489688  0.324321]]]\n"
     ]
    }
   ],
   "source": [
    "# actual data; note the trailing qualifier '.values' means we see actual latent heating numbers\n",
    "a, b, c, d, e, f = 0, 3, 15, 20, 33, 39\n",
    "print(ds['latent_heating'][0, a:b, c:d, e:f].values) # 0 is the only time index. Altitudes are 0, 0.5, 1.0 km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan  3. nan]\n",
      " [nan nan nan  1.  1.  1.]\n",
      " [nan nan nan nan  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.]\n",
      " [ 3.  1.  3.  3.  1.  1.]]\n",
      "\n",
      "\n",
      "[[-88. -88. -88. -88. 300. -88.]\n",
      " [-88. -88. -88. 140. 120. 140.]\n",
      " [-88. -88. -88. -88. 100. 100.]\n",
      " [140. 140. 120. 120. 120. 100.]\n",
      " [300. 120. 300. 300. 130. 120.]]\n",
      "\n",
      "\n",
      "[[0.      0.      0.      0.      0.      0.     ]\n",
      " [0.      0.      0.      0.32622 0.3464  0.29932]\n",
      " [0.      0.      0.      0.      1.02762 1.98917]\n",
      " [0.46025 0.39082 1.01581 1.27117 1.50243 1.35572]\n",
      " [0.      0.74239 0.      0.      2.94565 0.36834]]\n"
     ]
    }
   ],
   "source": [
    "# From these clearly surf_rain is the important parameter; Lauren is masking for non-zero / non-nan values only\n",
    "# The other two (rain_type and rain_type_original) look more like status flags\n",
    "print(ds['rain_type'][0, c:d, e:f].values)\n",
    "print('\\n')\n",
    "print(ds['rain_type_original'][0, c:d, e:f].values)\n",
    "print('\\n')\n",
    "print(ds['surf_rain'][0, c:d, e:f].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.5  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.\n",
      " 13.  14.  15.  16.  17. ] so ordinal element 16 is 14.0 \n",
      "\n",
      "dask.array<open_dataset-8f12d236d0952550a03a04a750df9922surf_rain, shape=(1, 56, 309), dtype=float32, chunksize=(1, 56, 309)>\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Don't yet support nd fancy indexing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-ae7992d6908e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# gives 1: print(len(dssr.data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdssr_nonzero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdssr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdssr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdssr_nonzero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# surf_r = surf_R[np.nonzero(surf_R)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dask/array/core.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'iu'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_with_int_dask_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_with_bool_dask_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dask/array/slicing.py\u001b[0m in \u001b[0;36mslice_with_int_dask_array\u001b[0;34m(x, index)\u001b[0m\n\u001b[1;32m    890\u001b[0m     ]\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfancy_indexes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Don't yet support nd fancy indexing)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0mout_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Don't yet support nd fancy indexing)"
     ]
    }
   ],
   "source": [
    "# ok one more thing to do... There are 56 x 309 (lat x lon) locations; how many have surf_rain > 0\n",
    "#   and what do those vertical latent heat profiles look like? This is done more thoroughly below in\n",
    "#   the original code. This is just for fun.\n",
    "\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "nlat, nlon = len(ds.latitude), len(ds.longitude)   # 56 x 309\n",
    "# print(nlat, nlon)\n",
    "\n",
    "# one altitude vector as a list suffices for all profiles; can also use .data\n",
    "alt = ds.altitude_lh.values\n",
    "print(alt, 'so ordinal element 16 is', alt[15], '\\n')\n",
    "\n",
    "# Lists of profile vectors (lh) or scalars: surface rain (sr), lat, lon\n",
    "lh = []\n",
    "sr = []\n",
    "lat = []\n",
    "lon = []\n",
    "\n",
    "dssr = ds.surf_rain.data         # this is attribute-style access, equivalent to dictionary style access ds['surf_rain']\n",
    "print(dssr)\n",
    "# The above without '.data' gives a DataArray. Using the .data qualifier gives a dask.array\n",
    "# gives 0.368...: print(float(dssr[0,19,38].values))\n",
    "# gives 'float': print(type(float(dssr[0,19,38].values)))\n",
    "# gives 1: print(len(dssr))\n",
    "# gives 1: print(len(dssr.data))\n",
    "\n",
    "dssr_nonzero = dssr[da.nonzero(dssr)]\n",
    "print(dssr_nonzero)\n",
    "# surf_r = surf_R[np.nonzero(surf_R)]\n",
    "\n",
    "# count = 0\n",
    "# for iLat in range(nlat):\n",
    "#     for iLon in range(nlon):\n",
    "#         v = float(dssr[0,iLat,iLon].values)\n",
    "#         if v > 0.: count += 1\n",
    "            \n",
    "# print(count)\n",
    "\n",
    "\n",
    "            \n",
    "# for i in range(nlat):\n",
    "#     print(i)\n",
    "#     for j in range(nlon):\n",
    "#         print(j)\n",
    "#         if (ds['surf_rain'][0, i, j].values > 0.):\n",
    "#             print('    ',i,j)\n",
    "#             sr.append(ds['surf_rain'][0,i,j].values)\n",
    "#             lat.append(ds['latitude'][i].values)\n",
    "#             lon.append(ds['longitude'][j].values)\n",
    "#             lh.append(ds['latent_heating'][0,:,i,j].values)\n",
    "            \n",
    "# print(len(sr))\n",
    "# print(sr[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lauren's original notebook picks up here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob                                            # glob is the UNIX file system pattern matching package\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file):\n",
    "    #Extract the data you want from file\n",
    "    altitude_lh = file.altitude_lh.data\n",
    "    surf_rain = file.surf_rain.data\n",
    "    latent_heating = file.latent_heating.data\n",
    "\n",
    "    lat = file.latitude.data\n",
    "    lon = file.longitude.data\n",
    "    time = file.time.data\n",
    "    \n",
    "    #create grid of altitude, lat, and lon coordinates\n",
    "    LAT, ALTITUDE, LON = np.meshgrid(lat, altitude_lh, lon)\n",
    "\n",
    "    #size of lat and lon as variables\n",
    "    nlat = len(lat)\n",
    "    nlon = len(lon)\n",
    "    nalt = len(altitude_lh)\n",
    "\n",
    "    #reshape as column vector (note the indicing is now column*ncolumns+row)\n",
    "    surf_rain = np.reshape(surf_rain,[nlat*nlon])\n",
    "    LH = np.reshape(latent_heating,[nalt,nlat*nlon])\n",
    "    ALTITUDE = np.reshape (ALTITUDE,[nalt,nlat*nlon])\n",
    "    LON = np.reshape (LON,[nalt,nlat*nlon])\n",
    "    LAT = np.reshape (LAT,[nalt,nlat*nlon])\n",
    "\n",
    "    #Remove values with NaN and zero rainfall\n",
    "    surf_R = surf_rain[~np.isnan(surf_rain)]\n",
    "    surf_r = surf_R[np.nonzero(surf_R)]\n",
    "\n",
    "    Lat_Heat = LH[:,~np.isnan(surf_rain)]\n",
    "    Lat_Heat = Lat_Heat[:,np.nonzero(surf_R)]\n",
    "    Lat_Heat = np.squeeze(Lat_Heat)\n",
    "\n",
    "    ALTITUDE = ALTITUDE[:,~np.isnan(surf_rain)]\n",
    "    ALTITUDE = ALTITUDE[:,np.nonzero(surf_R)]\n",
    "    ALTITUDE = np.squeeze(ALTITUDE)\n",
    "\n",
    "    LAT = LAT[:,~np.isnan(surf_rain)]\n",
    "    LAT = LAT[:,np.nonzero(surf_R)]\n",
    "    LAT = np.squeeze(LAT)\n",
    "\n",
    "    LON = LON[:,~np.isnan(surf_rain)]\n",
    "    LON = LON[:,np.nonzero(surf_R)]\n",
    "    LON = np.squeeze(LON)\n",
    "\n",
    "    #Remove any profiles where there is missing latent heat info\n",
    "    surf_r = surf_r[~pd.isnull(Lat_Heat).any(axis=0)]\n",
    "    LAT = LAT[:,~pd.isnull(Lat_Heat).any(axis=0)]\n",
    "    LON = LON[:,~pd.isnull(Lat_Heat).any(axis=0)]\n",
    "    ALTITUDE = ALTITUDE[:,~pd.isnull(Lat_Heat).any(axis=0)]\n",
    "    Lat_Heat = Lat_Heat[:,~pd.isnull(Lat_Heat).any(axis=0)]\n",
    "    Time = np.repeat(time,len(surf_r))\n",
    "    \n",
    "    return Lat_Heat.T, surf_r.T, ALTITUDE.T, LAT.T, LON.T, Time.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'months' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a5ced62ff929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##months = ['01','02','03','04','05','06','07','08','09','10','11','12']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mLat_Heat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msurf_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mALTITUDE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'months' is not defined"
     ]
    }
   ],
   "source": [
    "##months = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "for m in range(len(months)):\n",
    "    Lat_Heat = []\n",
    "    surf_r = []\n",
    "    ALTITUDE = []\n",
    "    LAT = []\n",
    "    LON = []\n",
    "    TIME = []\n",
    "    count = 0\n",
    "    for file in glob.glob(\"/Users/Lauren/Documents/NOAA/Precipitation/**/\"+months[m]+\"/*.nc4\", recursive=True):\n",
    "        L, S, A, la, lo, Ti = extract_data(xr.open_dataset(file))\n",
    "        if count==0:\n",
    "            Lat_Heat = L\n",
    "            ALTITUDE = A\n",
    "            LAT = la\n",
    "            LON = lo\n",
    "            TIME = Ti\n",
    "            count += 1\n",
    "            print(Lat_Heat.shape)\n",
    "        else:\n",
    "            Lat_Heat = np.concatenate((Lat_Heat,L),axis =0)\n",
    "            ALTITUDE = np.concatenate((ALTITUDE,A),axis =0)\n",
    "            LAT = np.concatenate((LAT,la),axis =0)\n",
    "            LON = np.concatenate((LON,lo),axis =0)\n",
    "            TIME = np.concatenate((TIME,Ti),axis =0)\n",
    "        surf_r = np.append(surf_r,S)\n",
    "    test = xr.Dataset(\n",
    "        data_vars = {'Latitude': (('time', 'altitude'),LAT), \n",
    "                     'Longitude': (('time', 'altitude'),LON), \n",
    "                     'Latent_Heat': (('time', 'altitude'), Lat_Heat),\n",
    "                     'surface_rain': (('time'), surf_r)},\n",
    "        coords = {'time': TIME,\n",
    "                 'altitude': ALTITUDE[0,:]})\n",
    "    print(test)\n",
    "    test.to_netcdf(path = \"EPO_1998_\"+months[m]+\".nc4\", compute = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LH = []\n",
    "SR = []\n",
    "Longitude = []\n",
    "Latitude = []\n",
    "count = 1\n",
    "for file in glob.glob(\"/Users/Lauren/Documents/NOAA/Precipitation/*.nc4\"):\n",
    "    ds = xr.open_dataset(file)\n",
    "    if count==1: \n",
    "        LH = ds.Latent_Heat.data\n",
    "        count +=1\n",
    "    else:\n",
    "        LH = np.concatenate((LH,ds.Latent_Heat.data),axis=0)\n",
    "    SR = np.append(SR,ds.surface_rain.data)\n",
    "    Latitude = np.append(Latitude,ds.Latitude.data[:,1])\n",
    "    Longitude = np.append(Longitude,ds.Longitude.data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the latent heat and rain rate at surface together for trainin data\n",
    "Xdata = np.concatenate((LH,SR.reshape(len(SR),1)),axis = 1)\n",
    "Xdata = Xdata[np.where(SR>5),:]\n",
    "Xdata = np.squeeze(Xdata)\n",
    "\n",
    "#divide by standard deviation to avoid one metric pulling harder than others\n",
    "MIN = np.min(Xdata,axis=0)\n",
    "MAX = np.max(Xdata,axis=0)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "Xdata_scaled = np.subtract(Xdata,MIN)\n",
    "Xdata_scaled = np.divide(Xdata_scaled,MAX-MIN)\n",
    "#Xdata_scaled[np.isnan(Xdata_scaled)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DBSCAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-87574e8514c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#model = KMeans(n_clusters=3, random_state=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(centers.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdata_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DBSCAN' is not defined"
     ]
    }
   ],
   "source": [
    "model = DBSCAN(eps=.05, min_samples=100)\n",
    "#model = KMeans(n_clusters=3, random_state=0)\n",
    "#print(centers.shape)\n",
    "model.fit(Xdata_scaled[0:100000,:])\n",
    "\n",
    "#centers = model.cluster_centers_\n",
    "labels = model.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(n_clusters_)\n",
    "print('Done')\n",
    "\n",
    "#plt.pcolormesh(centers[:,0:-1]*SDEV[None,0:-1])\n",
    "#plt.colorbar(orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xdata_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c8dc4a36f3d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXdata_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcat0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcat2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Xdata_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "test = Xdata_scaled[0:100000,:] \n",
    "cat0 = np.mean(test[labels==0,:],axis=0)\n",
    "cat1 = np.mean(test[labels==1,:],axis=0)\n",
    "cat2 = np.mean(test[labels==2,:],axis=0)\n",
    "\n",
    "print(test[labels==-1,:].shape)\n",
    "#print(test[labels==1,:].shape)\n",
    "print(test[labels==0,:].shape)\n",
    "\n",
    "plt.plot(test[labels==-1,:].T)\n",
    "\n",
    "\n",
    "#plt.plot(cat0)\n",
    "#plt.plot(cat1)\n",
    "#plt.plot(cat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels = model.labels_\n",
    "d = {'lat': Latitude, 'lon': Longitude, 'label': labels}\n",
    "d = pd.DataFrame(data=d)\n",
    "df = d.groupby(d.columns.tolist(),as_index=False).size()\n",
    "axes = np.array(df.axes)\n",
    "values = np.array(df.values)\n",
    "print(np.array(df.axes))\n",
    "print(np.array(df.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "lats = axes[:,:,0]\n",
    "lons = axes[:,:,1]\n",
    "cate = axes[:,:,2]\n",
    "\n",
    "\n",
    "# How much to zoom from coordinates (in degrees)\n",
    "zoom_scale = 3\n",
    "\n",
    "# Setup the bounding box for the zoom and bounds of the map\n",
    "bbox = [np.min(lats)-zoom_scale,np.max(lats)+zoom_scale,\\\n",
    "        np.min(lons)-zoom_scale,np.max(lons)+zoom_scale]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "# Define the projection, scale, the corners of the map, and the resolution.\n",
    "m = Basemap(projection='merc',llcrnrlat=bbox[0],urcrnrlat=bbox[1],\\\n",
    "            llcrnrlon=bbox[2],urcrnrlon=bbox[3],lat_ts=10,resolution='i')\n",
    "\n",
    "# Draw coastlines and fill continents and water with color\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='#CCCCCC',lake_color='lightblue')\n",
    "\n",
    "# draw parallels, meridians, and color boundaries\n",
    "m.drawparallels(np.arange(bbox[0],bbox[1],(bbox[1]-bbox[0])/5),labels=[1,0,0,0])\n",
    "m.drawmeridians(np.arange(bbox[2],bbox[3],(bbox[3]-bbox[2])/5),labels=[0,0,0,1],rotation=15)\n",
    "m.drawmapboundary(fill_color='lightblue')\n",
    "\n",
    "# format colors for elevation range\n",
    "alt_min = np.min(values)\n",
    "alt_max = np.max(values)\n",
    "cmap = plt.get_cmap('gist_earth')\n",
    "normalize = matplotlib.colors.Normalize(vmin=alt_min, vmax=alt_max)\n",
    "\n",
    "# plot elevations with different colors using the numpy interpolation mapping tool\n",
    "# the range [50,200] can be changed to create different colors and ranges\n",
    "for ii in range(0,len(values)):\n",
    "    x,y = m(lons[ii],lats[ii])\n",
    "    color_interp = np.interp(values[ii],[alt_min,alt_max],[50,200])\n",
    "    plt.plot(x,y,3,marker='o',color=cmap(int(color_interp)))\n",
    "\n",
    "# format the colorbar \n",
    "cax, _ = matplotlib.colorbar.make_axes(ax)\n",
    "cbar = matplotlib.colorbar.ColorbarBase(cax, cmap=cmap,norm=normalize,label='Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12550147392335598\n"
     ]
    }
   ],
   "source": [
    "test = SR[np.where(SR>5)]\n",
    "xdata = Xdata[np.where(SR>5),:]\n",
    "print(len(test)/len(SR))\n",
    "#plt.hist(SR, bins='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
